{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../study\"\n",
    "stats_path = folder_path + \"/stats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (19, 2044)\n",
      "X_train (10, 2042)\n",
      "X_test: (6, 2042)\n",
      "y_train (10,)\n",
      "y_test (6,)\n",
      "ID\n",
      "VNSLC_11    1\n",
      "VNSLC_10    1\n",
      "VNSLC_09    1\n",
      "VNSLC_23    0\n",
      "VNSLC_13    0\n",
      "VNSLC_04    1\n",
      "Name: resp, dtype: int64\n",
      "X_train After: (10, 1827)\n"
     ]
    }
   ],
   "source": [
    "from myTransformers import FilterSmall\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reading the whole dataset\n",
    "df = pd.read_csv(\"%s/dataset.csv\" % stats_path, index_col=\"ID\")\n",
    "print(\"Before:\", df.shape)\n",
    "\n",
    "# Removing the NaN values\n",
    "df = df.dropna(axis=0, how=\"any\")\n",
    "\n",
    "# Separing Data from labels and removing benzo and type\n",
    "X = df.drop([\"resp\", \"respPart\"], axis=1)\n",
    "y = df[\"resp\"]\n",
    "\n",
    "# splitting training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, shuffle=True, stratify=y)\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"y_test\", y_test.shape)\n",
    "print(y_test)\n",
    "\n",
    "# Removing to small dMRI features\n",
    "filtr = FilterSmall(1e-12, X_train.apply(lambda x : type(x[0]) == np.int64, axis=0))\n",
    "filtr.fit(X_train) \n",
    "X_train = pd.DataFrame(filtr.transform(X_train), index=X_train.index, columns=X_train.columns[filtr.get_support()])\n",
    "X_test = pd.DataFrame(filtr.transform(X_test), index=X_test.index, columns=X_test.columns[filtr.get_support()])\n",
    "\n",
    "col_dMRI = X_train.filter(regex=r'mean|std|skew|kurt').columns\n",
    "col_nTract = X_train.filter(regex=r'nTracts').columns\n",
    "col_cont = [\"age\", \"therapy_duration\", \"epilepsy_onset_age\", \"epilepsy_duration\", *col_nTract, *col_dMRI]\n",
    "col_disc = [\"sex\", \"AEDs\", \"benzo\", \"epilepsy_type\"]\n",
    "print(\"X_train After:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelName: linearSVM , scaler: StandardScaler , filter: f_classif\n",
      "linearSVM train score: 0.7666666666666666\n",
      "{'classifier__C': 1e-06, 'selection__k': 3}\n",
      "[[0 2]\n",
      " [0 4]]\n",
      "Balanced Accuracy Score 0.0\n",
      "Area Under ROC 0.5\n",
      "modelName: linearSVM , scaler: RobustScaler , filter: f_classif\n",
      "linearSVM train score: 0.7666666666666666\n",
      "{'classifier__C': 1e-06, 'selection__k': 3}\n",
      "[[0 2]\n",
      " [0 4]]\n",
      "Balanced Accuracy Score 0.0\n",
      "Area Under ROC 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet, LinearRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "def myScoreFunc(X, y):\n",
    "    mi_score = mutual_info_classif(X, y)\n",
    "    f_score = f_classif(X, y)[0]\n",
    "    return mi_score + f_score\n",
    "\n",
    "models = {\n",
    "    # SVM\n",
    "    \"linearSVM\" : (\n",
    "        LinearSVC(), \n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13, 17, 19),\n",
    "            \"classifier__C\" : (1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1),\n",
    "        }\n",
    "    ),\n",
    "    # \"SVM\" : (\n",
    "    #     SVC(),\n",
    "    #     [\n",
    "    #         {\n",
    "    #             \"selection__k\" : (2, 3, 5, 7, 9, 11, 13, 17, 19),\n",
    "    #             \"classifier__C\" :(1e-3, 1e-2, 1e-1, 1),\n",
    "    #             \"classifier__kernel\" : ['linear'], \n",
    "    #         },\n",
    "    #         {\n",
    "    #             \"selection__k\" : (2, 3, 5, 7, 9, 11, 13, 17, 19),\n",
    "    #             \"classifier__C\" :(1e-3, 1e-2, 1e-1, 1),\n",
    "    #             \"classifier__kernel\" : ['poly'],\n",
    "    #             \"classifier__degree\" : (2, 3, 4),\n",
    "    #         },\n",
    "    #         {\n",
    "    #             \"selection__k\" : (2, 3, 5, 7, 9, 11, 13, 17, 19),\n",
    "    #             \"classifier__C\" :(1e-3, 1e-2, 1e-1, 1),\n",
    "    #             \"classifier__kernel\" : ['rbf'], \n",
    "    #             \"classifier__gamma\" : (1e-3, 1e-2, 1e-1, 1),\n",
    "    #         },\n",
    "    #     ]\n",
    "    # ),\n",
    "    # # Linear \n",
    "    # \"LogReg\" : (\n",
    "    #     LogisticRegression(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13, 17, 19),\n",
    "    #         \"classifier__penalty\" : [\"l2\"],\n",
    "    #         \"classifier__dual\" : [True],\n",
    "    #         \"classifier__C\" :(1e-3, 1e-2, 1e-1),\n",
    "    #     },\n",
    "    # ),\n",
    "    # \"LinReg\" : (\n",
    "    #     LinearRegression(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13, 17, 19),\n",
    "    #     }\n",
    "    # ),\n",
    "    # \"ridgeReg\" : (\n",
    "    #     RidgeClassifier(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "    #         \"classifier__alpha\" : (0.5, 1, 5, 10, 20, 40)\n",
    "    #     }\n",
    "    # ),\n",
    "    # \"elasticNet\" : (\n",
    "    #     ElasticNet(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "    #         \"classifier__alpha\" : (0.5, 1, 5, 10, 20, 40),  # 0 == Linear Regression\n",
    "    #         \"classifier__l1_ratio\" : (0.5, 1), # 0 == Ridge Regression, 1 == Lasso Regression\n",
    "    #     },\n",
    "    # ),\n",
    "    # # Nearest Neighbors\n",
    "    # \"neighbors\" : (\n",
    "    #     KNeighborsClassifier(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "    #         \"classifier__n_neighbors\" : (2, 3, 5),\n",
    "    #         \"classifier__weights\" : (\"uniform\", \"distance\"),\n",
    "    #     }\n",
    "    # ),\n",
    "    # # Naive Bayes\n",
    "    # \"gaussianNaive\" : (\n",
    "    #     GaussianNB(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "    #     }\n",
    "    # ),\n",
    "    # \"multinomialNaive\" : (\n",
    "    #     MultinomialNB(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "    #     }\n",
    "    # ),\n",
    "    # \"complementNaive\" : (\n",
    "    #     ComplementNB(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "    #     }\n",
    "    # ),\n",
    "    # \"bernulliNaive\" : (\n",
    "    #     BernoulliNB(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "    #     }\n",
    "    # ),\n",
    "    # \"categoricalNaive\" : (\n",
    "    #     CategoricalNB(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "    #     }\n",
    "    # ),\n",
    "    # # Tree\n",
    "    # \"tree\" : (\n",
    "    #     DecisionTreeClassifier(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "    #         \"classifier__criterion\" : (\"gini\", \"entropy\", \"log_loss\")\n",
    "    #         # ccp_apha is a Regularization therm (to try)\n",
    "    #     }\n",
    "    # ),\n",
    "    # # Ensemble\n",
    "    # \"forest\" : (\n",
    "    #     RandomForestClassifier(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "    #         \"classifier__n_estimators\" : (100, 500, 1000),\n",
    "    #         # Check if make sense choose also the criterion (see the tree)\n",
    "    #         \"classifier__bootstrap\" : [True],\n",
    "    #         \"classifier__max_samples\" : [0.5],\n",
    "    #         \"classifier__max_features\" : [\"log2\"],\n",
    "    #         \"classifier__warm_star\" : [True],\n",
    "    #         \"classifier__oob_score\" : [True],\n",
    "    #         \"classifier__max_depth\" : [10, 20, 30],\n",
    "    #         # ccp_alpha da checkare\n",
    "    #     }\n",
    "    # ),\n",
    "    # \"extraForest\" : (\n",
    "    #     ExtraTreesClassifier(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "    #         \"classifier__n_estimators\" : (100, 500, 1000),\n",
    "    #         # Check if make sense choose also the criterion (see the tree)\n",
    "    #         \"classifier__bootstrap\" : [True],\n",
    "    #         \"classifier__max_samples\" : [0.5],\n",
    "    #         \"classifier__max_features\" : [\"log2\"],\n",
    "    #         \"classifier__warm_star\" : [True],\n",
    "    #         \"classifier__oob_score\" : [True],\n",
    "    #         \"classifier__max_depth\" : [10, 20, 30],\n",
    "    #         # ccp_alpha da checkare\n",
    "    #     }\n",
    "    # ),\n",
    "    # \"gradientBoosting\": (\n",
    "    #     GradientBoostingClassifier(),\n",
    "    #     {\n",
    "    #         \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "    #         \"classifier__n_estimators\" : (100, 200, 500, 1000),\n",
    "    #         \"classifier__learning_rate\" : (0.05, 0.1),\n",
    "    #         # Check if make sense choose also the criterion (see the tree)\n",
    "    #         \"classifier__max_samples\" : [0.5],\n",
    "    #         \"classifier__max_features\" : [\"log2\"],\n",
    "    #         \"classifier__warm_star\" : [True],\n",
    "    #         \"classifier__max_depth\" : [10, 20, 30],\n",
    "    #         \"classifier__validation_fraction\" : [0.20],\n",
    "    #         \"classifier__n_iter_no_change\" : [50],\n",
    "    #         # ccp_alpha da checkare\n",
    "    #     }\n",
    "    # )\n",
    "}\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=20, test_size=1/3)\n",
    "\n",
    "fitted_models = {}\n",
    "\n",
    "for modelName, (classifier, grid) in models.items():\n",
    "    for scaler in [StandardScaler(), RobustScaler()]:\n",
    "        for filter in [f_classif]:\n",
    "            print(\"modelName:\", modelName, \", scaler:\", scaler.__class__.__name__, \", filter:\", filter.__name__)\n",
    "\n",
    "            # Scaling\n",
    "            pre = ColumnTransformer(\n",
    "                    [(\"scaling\", scaler, col_cont)],\n",
    "                    remainder=\"passthrough\", # one hot or other stuff\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "            \n",
    "            # Remove costant values\n",
    "            # Yes, it's possible, since we have few data after the splitting and the cross validation is possible to have some features with same values. To remove them from the modeling we use the \n",
    "            varThres = VarianceThreshold()\n",
    "\n",
    "            # Feature Selection\n",
    "            selection = SelectKBest(\n",
    "                    score_func=filter,\n",
    "                )\n",
    "\n",
    "            # Pipe\n",
    "            pipe = Pipeline([\n",
    "                (\"pre\", pre),\n",
    "                (\"varThres\", varThres),\n",
    "                (\"selection\", selection),\n",
    "                (\"classifier\", classifier)\n",
    "            ])\n",
    "\n",
    "            model = GridSearchCV(\n",
    "                estimator=pipe,\n",
    "                param_grid=grid,\n",
    "                scoring=\"roc_auc\",\n",
    "                n_jobs=-1,\n",
    "                cv=sss,\n",
    "            )\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            fitted_models[modelName] = model\n",
    "\n",
    "            # Train\n",
    "            print(modelName, \"train score:\", model.best_score_)\n",
    "            print(model.best_params_) \n",
    "\n",
    "            # Test\n",
    "            y_pred = model.predict(X_test)\n",
    "            confusionMatrix = confusion_matrix(y_test, y_pred)  \n",
    "            print(confusionMatrix)  \n",
    "            print(\"Balanced Accuracy Score\", balanced_accuracy_score(y_test, y_pred, adjusted=True))\n",
    "            print(\"Area Under ROC\", roc_auc_score(y_test, y_pred,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier__C</th>\n",
       "      <th>selection__k</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>3</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>7</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>9</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>17</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>19</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    classifier__C  selection__k  Accuracy\n",
       "0        0.000001             2  0.733333\n",
       "1        0.000001             3  0.766667\n",
       "2        0.000001             5  0.733333\n",
       "3        0.000001             7  0.650000\n",
       "4        0.000001             9  0.700000\n",
       "..            ...           ...       ...\n",
       "58       1.000000             9  0.716667\n",
       "59       1.000000            11  0.700000\n",
       "60       1.000000            13  0.666667\n",
       "61       1.000000            17  0.666667\n",
       "62       1.000000            19  0.683333\n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dMRI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
