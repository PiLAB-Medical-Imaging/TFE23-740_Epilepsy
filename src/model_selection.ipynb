{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "stats_path = \"./drive/MyDrive/Epilepsy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../study\"\n",
    "stats_path = folder_path + \"/stats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (19, 2042)\n",
      "y: (19,)\n"
     ]
    }
   ],
   "source": [
    "# Reading the whole dataset\n",
    "df = pd.read_csv(\"%s/dataset.csv\" % stats_path, index_col=\"ID\")\n",
    "\n",
    "# Separing Data from targets\n",
    "X = df.drop([\"resp\", \"respPart\"], axis=1)\n",
    "y = df[\"resp\"]\n",
    "\n",
    "col_dMRI = X.filter(regex=r'mean|std|skew|kurt').columns\n",
    "col_nTract = X.filter(regex=r'nTracts').columns\n",
    "col_cont = [\"age\", \"therapy_duration\", \"epilepsy_onset_age\", \"epilepsy_duration\", *col_nTract, *col_dMRI]\n",
    "col_disc = [\"sex\", \"AEDs\", \"benzo\", \"epilepsy_type\"]\n",
    "\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15  8  6 16 10  9 14  4 12  2  3 11  0 13 17] [ 1 18  5  7]\n",
      "[15 14  2 11  8  5  1 17  0  6 10 18 16  9  3] [ 4 12  7 13]\n",
      "[13  6 11 12 16  8  1 18  3 14  9  2  4 10 17] [ 7  5 15  0]\n",
      "[11 13  5 18 12 15  4  0 14  9  3  2 17  6 10] [ 8 16  1  7]\n",
      "[17  3 11  4 13  0  7  2 18  6 16 10  5 14  8] [ 1 12 15  9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "seed = 9\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=1/5, random_state=seed)\n",
    "for train, test in sss.split(X, y):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedOuter = 7\n",
    "sssOuter = StratifiedShuffleSplit(n_splits=5, test_size=1/5, random_state=seed)\n",
    "seedInner = 13\n",
    "sssInner = StratifiedShuffleSplit(n_splits=5, test_size=1/5, random_state=seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram-based Gradient Boosting Classification Tree\n",
    "\n",
    "This is a classififier similar to the **Gradient Boosting Classifier**, but it can work also with features that have NaN values. The implementation is based on [LightGBM](https://github.com/Microsoft/LightGBM). It's much faster than the normal implementation of the radient Boosting.\n",
    "\n",
    "For binary classification is used a ```log_loss``` as loss for classification.\n",
    "The number of bins is controlled by ```max_bins```. Using less bins acts as a form of regularization. It is generally reccomended to use as many bins as possible, which is the default.\n",
    "The ```l2_regularization``` is a regulariazion term, and correspond to $\\lambda$.\n",
    "The easrly-stopping is controlled by ```early_stopping```, ```scoring```, ```validation_fraction```, ```n_iter_no_change```, and ```tol```. \n",
    "\n",
    "The algorithm has native support for categorical features. To enable this support, a boolean mask can be passed to ```categorical_feature```, indicating which feature is categorical. Or, one can pass a list of integers indicating the indices of the categorical features.\n",
    "\n",
    "Can be declared some constraints to speed-up the algorithm: *Monotonic Constraint*, and *Interaction constraint*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler: StandardScaler , filter: Recoursive\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 141\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m# Pipe\u001b[39;00m\n\u001b[1;32m    134\u001b[0m pipe \u001b[39m=\u001b[39m Pipeline([\n\u001b[1;32m    135\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mpre\u001b[39m\u001b[39m\"\u001b[39m, pre),\n\u001b[1;32m    136\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mvarThres\u001b[39m\u001b[39m\"\u001b[39m, varThres),\n\u001b[1;32m    137\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mselection\u001b[39m\u001b[39m\"\u001b[39m, selection),\n\u001b[1;32m    138\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mhist\u001b[39m\u001b[39m\"\u001b[39m, hist)\n\u001b[1;32m    139\u001b[0m ])\n\u001b[0;32m--> 141\u001b[0m estimate_score_NestedCV(pipe, X, y, hyperPars, sssOuter, sssInner)\n",
      "Cell \u001b[0;32mIn[127], line 25\u001b[0m, in \u001b[0;36mestimate_score_NestedCV\u001b[0;34m(estimator, X, y, hyperPars, cvOuter, cvInner)\u001b[0m\n\u001b[1;32m     15\u001b[0m model \u001b[39m=\u001b[39m GridSearchCV(\n\u001b[1;32m     16\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m     17\u001b[0m     param_grid\u001b[39m=\u001b[39mhyperPars,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     refit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[39m# Outer Cross-Validaton for the estimation of the score\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m scores \u001b[39m=\u001b[39m cross_val_score(\n\u001b[1;32m     26\u001b[0m     model, X, y,\n\u001b[1;32m     27\u001b[0m     scoring\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mroc_auc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m     cv\u001b[39m=\u001b[39;49mcvOuter,\n\u001b[1;32m     29\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mROC-AUC: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (np\u001b[39m.\u001b[39mmean(scores), np\u001b[39m.\u001b[39mstd(scores)))\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m----------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif, VarianceThreshold, SelectPercentile, SequentialFeatureSelector, RFECV, SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "def myScoreFunc(X, y):\n",
    "    mi_score = mutual_info_classif(X, y)\n",
    "    f_score = f_classif(X, y)[0]\n",
    "    return mi_score + f_score\n",
    "\n",
    "def estimate_score_NestedCV(estimator, X, y, hyperPars, cvOuter, cvInner):\n",
    "    # Inner Cross-Validation for Hyper-parameters selection\n",
    "    model = GridSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_grid=hyperPars,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=-1,\n",
    "        cv=cvInner,\n",
    "        refit=True\n",
    "    )\n",
    "\n",
    "    # Outer Cross-Validaton for the estimation of the score\n",
    "    scores = cross_val_score(\n",
    "        model, X, y,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=cvOuter,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    print('ROC-AUC: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
    "    print(\"----------------------------\")\n",
    "\n",
    "hyperPars = {\n",
    "    \"hist__learning_rate\" : [0.01, 0.1],\n",
    "    \"hist__max_iter\" : [100, 200],\n",
    "    \"hist__max_depth\" : [8, 16, 32],\n",
    "    \"hist__l2_regularization\" : [0, 0.1, 1],\n",
    "}\n",
    "\n",
    "models = []\n",
    "\n",
    "# Without feature Selection\n",
    "for scaler in [StandardScaler(), RobustScaler()]:\n",
    "    print(\"Scaler:\", scaler.__class__.__name__, \", filter: No\")\n",
    "\n",
    "    # Scaling\n",
    "    pre = ColumnTransformer(\n",
    "        [(\"scaling\", scaler, col_cont)],\n",
    "        remainder=\"passthrough\", # one hot or other stuff\n",
    "    )\n",
    "\n",
    "    mask_cat = np.array([False,]*X.shape[1])\n",
    "    mask_cat[-4:] = True\n",
    "    hist = HistGradientBoostingClassifier(\n",
    "        categorical_features = mask_cat\n",
    "    )\n",
    "\n",
    "    # Pipe\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", pre),\n",
    "        (\"hist\", hist)\n",
    "    ])\n",
    "\n",
    "    estimate_score_NestedCV(pipe, X, y, hyperPars, sssOuter, sssInner)\n",
    "\n",
    "hyperPars[\"selection__n_features_to_select\"] = [5, 10, 15]\n",
    "\n",
    "# Forward method \n",
    "for scaler in [StandardScaler(), RobustScaler()]:\n",
    "    print(\"Scaler:\", scaler.__class__.__name__, \", filter: Forward\")\n",
    "\n",
    "    # Scaling\n",
    "    pre = ColumnTransformer(\n",
    "            [(\"scaling\", scaler, col_cont)],\n",
    "            remainder=\"passthrough\", # one hot or other stuff\n",
    "        )\n",
    "    \n",
    "    # Remove costant values\n",
    "    # Yes, it's possible, since we have few data after the splitting and the cross validation is possible to have some features with same values. To remove them from the modeling we use the \n",
    "    varThres = VarianceThreshold()\n",
    "\n",
    "    hist = HistGradientBoostingClassifier()\n",
    "\n",
    "    # Feature Selection\n",
    "    selection = SequentialFeatureSelector(\n",
    "        estimator = hist, \n",
    "        direction=\"forward\",\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=sss,\n",
    "    )\n",
    "\n",
    "    # Pipe\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", pre),\n",
    "        (\"varThres\", varThres),\n",
    "        (\"selection\", selection),\n",
    "        (\"hist\", hist)\n",
    "    ])\n",
    "\n",
    "    estimate_score_NestedCV(pipe, X, y, hyperPars, sssOuter, sssInner)\n",
    "\n",
    "del hyperPars[\"selection__n_features_to_select\"]\n",
    "hyperPars[\"selection__min_features_to_select\"] = [2]\n",
    "\n",
    "# Recoursive method \n",
    "for scaler in [StandardScaler(), RobustScaler()]:\n",
    "    print(\"Scaler:\", scaler.__class__.__name__, \", filter: Recoursive\")\n",
    "\n",
    "    # Scaling\n",
    "    pre = ColumnTransformer(\n",
    "            [(\"scaling\", scaler, col_cont)],\n",
    "            remainder=\"passthrough\", # one hot or other stuff\n",
    "        )\n",
    "    \n",
    "    # Remove costant values\n",
    "    # Yes, it's possible, since we have few data after the splitting and the cross validation is possible to have some features with same values. To remove them from the modeling we use the \n",
    "    varThres = VarianceThreshold()\n",
    "\n",
    "    hist = HistGradientBoostingClassifier()\n",
    "\n",
    "    # Feature Selection\n",
    "    selection = RFECV(\n",
    "        estimator=hist,\n",
    "        step=1,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=sss,\n",
    "        min_features_to_select=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Pipe\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", pre),\n",
    "        (\"varThres\", varThres),\n",
    "        (\"selection\", selection),\n",
    "        (\"hist\", hist)\n",
    "    ])\n",
    "\n",
    "    estimate_score_NestedCV(pipe, X, y, hyperPars, sssOuter, sssInner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelName: linearSVM , scaler: StandardScaler , filter: f_classif\n",
      "linearSVM train score: 0.8333333333333334\n",
      "{'classifier__C': 1e-06, 'selection__k': 17}\n",
      "                                               mean_test_score   \n",
      "{'classifier__C': 0.0001, 'selection__k': 19}         0.833333  \\\n",
      "{'classifier__C': 0.0001, 'selection__k': 17}         0.833333   \n",
      "{'classifier__C': 0.001, 'selection__k': 17}          0.833333   \n",
      "{'classifier__C': 1e-06, 'selection__k': 17}          0.833333   \n",
      "{'classifier__C': 1e-06, 'selection__k': 19}          0.833333   \n",
      "{'classifier__C': 0.001, 'selection__k': 19}          0.833333   \n",
      "{'classifier__C': 0.01, 'selection__k': 19}           0.833333   \n",
      "{'classifier__C': 0.01, 'selection__k': 17}           0.833333   \n",
      "{'classifier__C': 1e-05, 'selection__k': 19}          0.833333   \n",
      "{'classifier__C': 1e-05, 'selection__k': 17}          0.833333   \n",
      "\n",
      "                                               std_test_score  rank_test_score  \n",
      "{'classifier__C': 0.0001, 'selection__k': 19}        0.235702                1  \n",
      "{'classifier__C': 0.0001, 'selection__k': 17}        0.235702                1  \n",
      "{'classifier__C': 0.001, 'selection__k': 17}         0.235702                1  \n",
      "{'classifier__C': 1e-06, 'selection__k': 17}         0.235702                1  \n",
      "{'classifier__C': 1e-06, 'selection__k': 19}         0.235702                1  \n",
      "{'classifier__C': 0.001, 'selection__k': 19}         0.235702                1  \n",
      "{'classifier__C': 0.01, 'selection__k': 19}          0.235702                1  \n",
      "{'classifier__C': 0.01, 'selection__k': 17}          0.235702                1  \n",
      "{'classifier__C': 1e-05, 'selection__k': 19}         0.235702                1  \n",
      "{'classifier__C': 1e-05, 'selection__k': 17}         0.235702                1  \n",
      "[[1 1]\n",
      " [1 3]]\n",
      "Balanced Accuracy Score 0.25\n",
      "Area Under ROC 0.625\n",
      "modelName: linearSVM , scaler: RobustScaler , filter: f_classif\n",
      "linearSVM train score: 0.7777777777777777\n",
      "{'classifier__C': 1e-06, 'selection__k': 5}\n",
      "                                              mean_test_score  std_test_score   \n",
      "{'classifier__C': 0.1, 'selection__k': 7}            0.777778         0.31427  \\\n",
      "{'classifier__C': 0.0001, 'selection__k': 5}         0.777778         0.31427   \n",
      "{'classifier__C': 0.1, 'selection__k': 5}            0.777778         0.31427   \n",
      "{'classifier__C': 0.1, 'selection__k': 9}            0.777778         0.31427   \n",
      "{'classifier__C': 0.01, 'selection__k': 9}           0.777778         0.31427   \n",
      "{'classifier__C': 0.001, 'selection__k': 5}          0.777778         0.31427   \n",
      "{'classifier__C': 1, 'selection__k': 5}              0.777778         0.31427   \n",
      "{'classifier__C': 1e-05, 'selection__k': 5}          0.777778         0.31427   \n",
      "{'classifier__C': 1, 'selection__k': 9}              0.777778         0.31427   \n",
      "{'classifier__C': 0.01, 'selection__k': 5}           0.777778         0.31427   \n",
      "\n",
      "                                              rank_test_score  \n",
      "{'classifier__C': 0.1, 'selection__k': 7}                   1  \n",
      "{'classifier__C': 0.0001, 'selection__k': 5}                1  \n",
      "{'classifier__C': 0.1, 'selection__k': 5}                   1  \n",
      "{'classifier__C': 0.1, 'selection__k': 9}                   1  \n",
      "{'classifier__C': 0.01, 'selection__k': 9}                  1  \n",
      "{'classifier__C': 0.001, 'selection__k': 5}                 1  \n",
      "{'classifier__C': 1, 'selection__k': 5}                     1  \n",
      "{'classifier__C': 1e-05, 'selection__k': 5}                 1  \n",
      "{'classifier__C': 1, 'selection__k': 9}                     1  \n",
      "{'classifier__C': 0.01, 'selection__k': 5}                  1  \n",
      "[[1 1]\n",
      " [1 3]]\n",
      "Balanced Accuracy Score 0.25\n",
      "Area Under ROC 0.625\n",
      "modelName: SVM , scaler: StandardScaler , filter: f_classif\n",
      "SVM train score: 1.0\n",
      "{'classifier__C': 0.001, 'classifier__kernel': 'linear', 'selection__k': 13}\n",
      "                                                    mean_test_score   \n",
      "{'classifier__C': 0.1, 'classifier__gamma': 0.0...              1.0  \\\n",
      "{'classifier__C': 0.1, 'classifier__gamma': 0.0...              1.0   \n",
      "{'classifier__C': 1, 'classifier__gamma': 0.001...              1.0   \n",
      "{'classifier__C': 1, 'classifier__gamma': 0.001...              1.0   \n",
      "{'classifier__C': 0.1, 'classifier__kernel': 'l...              1.0   \n",
      "{'classifier__C': 1, 'classifier__degree': 3, '...              1.0   \n",
      "{'classifier__C': 1, 'classifier__degree': 3, '...              1.0   \n",
      "{'classifier__C': 0.01, 'classifier__gamma': 0....              1.0   \n",
      "{'classifier__C': 0.01, 'classifier__kernel': '...              1.0   \n",
      "{'classifier__C': 0.01, 'classifier__kernel': '...              1.0   \n",
      "\n",
      "                                                    std_test_score   \n",
      "{'classifier__C': 0.1, 'classifier__gamma': 0.0...             0.0  \\\n",
      "{'classifier__C': 0.1, 'classifier__gamma': 0.0...             0.0   \n",
      "{'classifier__C': 1, 'classifier__gamma': 0.001...             0.0   \n",
      "{'classifier__C': 1, 'classifier__gamma': 0.001...             0.0   \n",
      "{'classifier__C': 0.1, 'classifier__kernel': 'l...             0.0   \n",
      "{'classifier__C': 1, 'classifier__degree': 3, '...             0.0   \n",
      "{'classifier__C': 1, 'classifier__degree': 3, '...             0.0   \n",
      "{'classifier__C': 0.01, 'classifier__gamma': 0....             0.0   \n",
      "{'classifier__C': 0.01, 'classifier__kernel': '...             0.0   \n",
      "{'classifier__C': 0.01, 'classifier__kernel': '...             0.0   \n",
      "\n",
      "                                                    rank_test_score  \n",
      "{'classifier__C': 0.1, 'classifier__gamma': 0.0...                1  \n",
      "{'classifier__C': 0.1, 'classifier__gamma': 0.0...                1  \n",
      "{'classifier__C': 1, 'classifier__gamma': 0.001...                1  \n",
      "{'classifier__C': 1, 'classifier__gamma': 0.001...                1  \n",
      "{'classifier__C': 0.1, 'classifier__kernel': 'l...                1  \n",
      "{'classifier__C': 1, 'classifier__degree': 3, '...                1  \n",
      "{'classifier__C': 1, 'classifier__degree': 3, '...                1  \n",
      "{'classifier__C': 0.01, 'classifier__gamma': 0....                1  \n",
      "{'classifier__C': 0.01, 'classifier__kernel': '...                1  \n",
      "{'classifier__C': 0.01, 'classifier__kernel': '...                1  \n",
      "[[0 2]\n",
      " [0 4]]\n",
      "Balanced Accuracy Score 0.0\n",
      "Area Under ROC 0.5\n",
      "modelName: SVM , scaler: RobustScaler , filter: f_classif\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 221\u001b[0m\n\u001b[1;32m    206\u001b[0m pipe \u001b[39m=\u001b[39m Pipeline([\n\u001b[1;32m    207\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mpre\u001b[39m\u001b[39m\"\u001b[39m, pre),\n\u001b[1;32m    208\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mvarThres\u001b[39m\u001b[39m\"\u001b[39m, varThres),\n\u001b[1;32m    209\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mselection\u001b[39m\u001b[39m\"\u001b[39m, selection),\n\u001b[1;32m    210\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mclassifier\u001b[39m\u001b[39m\"\u001b[39m, classifier)\n\u001b[1;32m    211\u001b[0m ])\n\u001b[1;32m    213\u001b[0m model \u001b[39m=\u001b[39m GridSearchCV(\n\u001b[1;32m    214\u001b[0m     estimator\u001b[39m=\u001b[39mpipe,\n\u001b[1;32m    215\u001b[0m     param_grid\u001b[39m=\u001b[39mgrid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     cv\u001b[39m=\u001b[39mskf,\n\u001b[1;32m    219\u001b[0m )\n\u001b[0;32m--> 221\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m    222\u001b[0m fitted_models[modelName] \u001b[39m=\u001b[39m model\n\u001b[1;32m    224\u001b[0m \u001b[39m# Train\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/dMRI/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet, LinearRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "def myScoreFunc(X, y):\n",
    "    mi_score = mutual_info_classif(X, y)\n",
    "    f_score = f_classif(X, y)[0]\n",
    "    return mi_score + f_score\n",
    "\n",
    "models = {\n",
    "    # SVM\n",
    "    \"linearSVM\" : (\n",
    "        LinearSVC(), \n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13, 17, 19),\n",
    "            \"classifier__C\" : (1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1),\n",
    "        }\n",
    "    ),\n",
    "    \"SVM\" : (\n",
    "        SVC(),\n",
    "        [\n",
    "            {\n",
    "                \"selection__k\" : (2, 3, 5, 7, 9, 11, 13, 17, 19),\n",
    "                \"classifier__C\" :(1e-3, 1e-2, 1e-1, 1),\n",
    "                \"classifier__kernel\" : ['linear'], \n",
    "            },\n",
    "            {\n",
    "                \"selection__k\" : (2, 3, 5, 7, 9, 11, 13, 17, 19),\n",
    "                \"classifier__C\" :(1e-3, 1e-2, 1e-1, 1),\n",
    "                \"classifier__kernel\" : ['poly'],\n",
    "                \"classifier__degree\" : (2, 3, 4),\n",
    "            },\n",
    "            {\n",
    "                \"selection__k\" : (2, 3, 5, 7, 9, 11, 13, 17, 19),\n",
    "                \"classifier__C\" :(1e-3, 1e-2, 1e-1, 1),\n",
    "                \"classifier__kernel\" : ['rbf'], \n",
    "                \"classifier__gamma\" : (1e-3, 1e-2, 1e-1, 1),\n",
    "            },\n",
    "        ]\n",
    "    ),\n",
    "    # Linear \n",
    "    \"LogReg\" : (\n",
    "        LogisticRegression(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13, 17, 19),\n",
    "            \"classifier__penalty\" : [\"l2\"],\n",
    "            \"classifier__dual\" : [True],\n",
    "            \"classifier__C\" :(1e-3, 1e-2, 1e-1),\n",
    "        },\n",
    "    ),\n",
    "    \"LinReg\" : (\n",
    "        LinearRegression(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13, 17, 19),\n",
    "        }\n",
    "    ),\n",
    "    \"ridgeReg\" : (\n",
    "        RidgeClassifier(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "            \"classifier__alpha\" : (0.5, 1, 5, 10, 20, 40)\n",
    "        }\n",
    "    ),\n",
    "    \"elasticNet\" : (\n",
    "        ElasticNet(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "            \"classifier__alpha\" : (0.5, 1, 5, 10, 20, 40),  # 0 == Linear Regression\n",
    "            \"classifier__l1_ratio\" : (0.5, 1), # 0 == Ridge Regression, 1 == Lasso Regression\n",
    "        },\n",
    "    ),\n",
    "    # Nearest Neighbors\n",
    "    \"neighbors\" : (\n",
    "        KNeighborsClassifier(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "            \"classifier__n_neighbors\" : (2, 3, 5),\n",
    "            \"classifier__weights\" : (\"uniform\", \"distance\"),\n",
    "        }\n",
    "    ),\n",
    "    # Naive Bayes\n",
    "    \"gaussianNaive\" : (\n",
    "        GaussianNB(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "        }\n",
    "    ),\n",
    "    \"multinomialNaive\" : (\n",
    "        MultinomialNB(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "        }\n",
    "    ),\n",
    "    \"complementNaive\" : (\n",
    "        ComplementNB(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "        }\n",
    "    ),\n",
    "    \"bernulliNaive\" : (\n",
    "        BernoulliNB(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "        }\n",
    "    ),\n",
    "    \"categoricalNaive\" : (\n",
    "        CategoricalNB(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "        }\n",
    "    ),\n",
    "    # Tree\n",
    "    \"tree\" : (\n",
    "        DecisionTreeClassifier(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "            \"classifier__criterion\" : (\"gini\", \"entropy\", \"log_loss\")\n",
    "            # ccp_apha is a Regularization therm (to try)\n",
    "        }\n",
    "    ),\n",
    "    # Ensemble\n",
    "    \"forest\" : (\n",
    "        RandomForestClassifier(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "            \"classifier__n_estimators\" : (100, 500, 1000),\n",
    "            # Check if make sense choose also the criterion (see the tree)\n",
    "            \"classifier__bootstrap\" : [True],\n",
    "            \"classifier__max_samples\" : [0.5],\n",
    "            \"classifier__max_features\" : [\"log2\"],\n",
    "            \"classifier__warm_star\" : [True],\n",
    "            \"classifier__oob_score\" : [True],\n",
    "            \"classifier__max_depth\" : [10, 20, 30],\n",
    "            # ccp_alpha da checkare\n",
    "        }\n",
    "    ),\n",
    "    \"extraForest\" : (\n",
    "        ExtraTreesClassifier(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "            \"classifier__n_estimators\" : (100, 500, 1000),\n",
    "            # Check if make sense choose also the criterion (see the tree)\n",
    "            \"classifier__bootstrap\" : [True],\n",
    "            \"classifier__max_samples\" : [0.5],\n",
    "            \"classifier__max_features\" : [\"log2\"],\n",
    "            \"classifier__warm_star\" : [True],\n",
    "            \"classifier__oob_score\" : [True],\n",
    "            \"classifier__max_depth\" : [10, 20, 30],\n",
    "            # ccp_alpha da checkare\n",
    "        }\n",
    "    ),\n",
    "    \"gradientBoosting\": (\n",
    "        GradientBoostingClassifier(),\n",
    "        {\n",
    "            \"selection__k\" : (2, 3, 5, 7, 9, 11, 13),\n",
    "            \"classifier__n_estimators\" : (100, 200, 500, 1000),\n",
    "            \"classifier__learning_rate\" : (0.05, 0.1),\n",
    "            # Check if make sense choose also the criterion (see the tree)\n",
    "            \"classifier__max_samples\" : [0.5],\n",
    "            \"classifier__max_features\" : [\"log2\"],\n",
    "            \"classifier__warm_star\" : [True],\n",
    "            \"classifier__max_depth\" : [10, 20, 30],\n",
    "            \"classifier__validation_fraction\" : [0.20],\n",
    "            \"classifier__n_iter_no_change\" : [50],\n",
    "            # ccp_alpha da checkare\n",
    "        }\n",
    "    )\n",
    "}\n",
    "\n",
    "# It's important to evaluate each algorithm with the same training data e test data\n",
    "seed = 7\n",
    "sss = StratifiedShuffleSplit(n_splits=20, test_size=1/3, random_state=seed)\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "fitted_models = {}\n",
    "\n",
    "for modelName, (classifier, grid) in models.items():\n",
    "    for scaler in [StandardScaler(), RobustScaler()]:\n",
    "        for filter in [f_classif, mutual_info_classif, myScoreFunc]:\n",
    "            print(\"modelName:\", modelName, \", scaler:\", scaler.__class__.__name__, \", filter:\", filter.__name__)\n",
    "\n",
    "            # Scaling\n",
    "            pre = ColumnTransformer(\n",
    "                    [(\"scaling\", scaler, col_cont)],\n",
    "                    remainder=\"passthrough\", # one hot or other stuff\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "            \n",
    "            # Remove costant values\n",
    "            # Yes, it's possible, since we have few data after the splitting and the cross validation is possible to have some features with same values. To remove them from the modeling we use the \n",
    "            varThres = VarianceThreshold()\n",
    "\n",
    "            # Feature Selection\n",
    "            selection = SelectKBest(\n",
    "                    score_func=filter,\n",
    "                )\n",
    "\n",
    "            # Pipe\n",
    "            pipe = Pipeline([\n",
    "                (\"pre\", pre),\n",
    "                (\"varThres\", varThres),\n",
    "                (\"selection\", selection),\n",
    "                (\"classifier\", classifier)\n",
    "            ])\n",
    "\n",
    "            model = GridSearchCV(\n",
    "                estimator=pipe,\n",
    "                param_grid=grid,\n",
    "                scoring=\"roc_auc\",\n",
    "                n_jobs=-1,\n",
    "                cv=skf,\n",
    "            )\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            fitted_models[modelName] = model\n",
    "\n",
    "            # Train\n",
    "            print(modelName, \"train score:\", model.best_score_)\n",
    "            print(model.best_params_) \n",
    "\n",
    "            idx = [param.__str__() for param in model.cv_results_[\"params\"]]\n",
    "            results = pd.DataFrame(pd.concat([pd.DataFrame(model.cv_results_[\"mean_test_score\"], index=idx, columns=[\"mean_test_score\"]), pd.DataFrame(model.cv_results_[\"std_test_score\"], index=idx, columns=[\"std_test_score\"]), pd.DataFrame(model.cv_results_[\"rank_test_score\"], index=idx, columns=[\"rank_test_score\"])], axis=1))\n",
    "            print(results.sort_values(\"rank_test_score\")[:10])\n",
    "\n",
    "            # Test\n",
    "            y_pred = model.predict(X_test)\n",
    "            confusionMatrix = confusion_matrix(y_test, y_pred)  \n",
    "            print(confusionMatrix)  \n",
    "            print(\"Balanced Accuracy Score\", balanced_accuracy_score(y_test, y_pred))\n",
    "            print(\"Area Under ROC\", roc_auc_score(y_test, y_pred,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              mean_test_score  std_test_score   \n",
      "{'classifier__C': 0.001, 'selection__k': 3}          0.766667        0.334996  \\\n",
      "{'classifier__C': 1e-06, 'selection__k': 3}          0.766667        0.334996   \n",
      "{'classifier__C': 0.01, 'selection__k': 3}           0.766667        0.334996   \n",
      "{'classifier__C': 0.0001, 'selection__k': 3}         0.766667        0.334996   \n",
      "{'classifier__C': 1e-05, 'selection__k': 3}          0.766667        0.334996   \n",
      "{'classifier__C': 0.1, 'selection__k': 3}            0.766667        0.334996   \n",
      "{'classifier__C': 1, 'selection__k': 5}              0.750000        0.314024   \n",
      "{'classifier__C': 0.001, 'selection__k': 5}          0.733333        0.326599   \n",
      "{'classifier__C': 0.01, 'selection__k': 2}           0.733333        0.359011   \n",
      "{'classifier__C': 0.01, 'selection__k': 5}           0.733333        0.326599   \n",
      "\n",
      "                                              rank_test_score  \n",
      "{'classifier__C': 0.001, 'selection__k': 3}                 1  \n",
      "{'classifier__C': 1e-06, 'selection__k': 3}                 1  \n",
      "{'classifier__C': 0.01, 'selection__k': 3}                  1  \n",
      "{'classifier__C': 0.0001, 'selection__k': 3}                1  \n",
      "{'classifier__C': 1e-05, 'selection__k': 3}                 1  \n",
      "{'classifier__C': 0.1, 'selection__k': 3}                   1  \n",
      "{'classifier__C': 1, 'selection__k': 5}                     7  \n",
      "{'classifier__C': 0.001, 'selection__k': 5}                 8  \n",
      "{'classifier__C': 0.01, 'selection__k': 2}                  8  \n",
      "{'classifier__C': 0.01, 'selection__k': 5}                  8  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx = [param.__str__() for param in model.cv_results_[\"params\"]]\n",
    "grid = pd.DataFrame(pd.concat([pd.DataFrame(model.cv_results_[\"mean_test_score\"], index=idx, columns=[\"mean_test_score\"]), pd.DataFrame(model.cv_results_[\"std_test_score\"], index=idx, columns=[\"std_test_score\"]), pd.DataFrame(model.cv_results_[\"rank_test_score\"], index=idx, columns=[\"rank_test_score\"])], axis=1))\n",
    "print(grid.sort_values(\"rank_test_score\")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"{'classifier__C': 1e-06, 'selection__k': 2}\",\n",
       " \"{'classifier__C': 1e-06, 'selection__k': 3}\",\n",
       " \"{'classifier__C': 1e-06, 'selection__k': 5}\",\n",
       " \"{'classifier__C': 1e-06, 'selection__k': 7}\",\n",
       " \"{'classifier__C': 1e-06, 'selection__k': 9}\",\n",
       " \"{'classifier__C': 1e-06, 'selection__k': 11}\",\n",
       " \"{'classifier__C': 1e-06, 'selection__k': 13}\",\n",
       " \"{'classifier__C': 1e-06, 'selection__k': 17}\",\n",
       " \"{'classifier__C': 1e-06, 'selection__k': 19}\",\n",
       " \"{'classifier__C': 1e-05, 'selection__k': 2}\",\n",
       " \"{'classifier__C': 1e-05, 'selection__k': 3}\",\n",
       " \"{'classifier__C': 1e-05, 'selection__k': 5}\",\n",
       " \"{'classifier__C': 1e-05, 'selection__k': 7}\",\n",
       " \"{'classifier__C': 1e-05, 'selection__k': 9}\",\n",
       " \"{'classifier__C': 1e-05, 'selection__k': 11}\",\n",
       " \"{'classifier__C': 1e-05, 'selection__k': 13}\",\n",
       " \"{'classifier__C': 1e-05, 'selection__k': 17}\",\n",
       " \"{'classifier__C': 1e-05, 'selection__k': 19}\",\n",
       " \"{'classifier__C': 0.0001, 'selection__k': 2}\",\n",
       " \"{'classifier__C': 0.0001, 'selection__k': 3}\",\n",
       " \"{'classifier__C': 0.0001, 'selection__k': 5}\",\n",
       " \"{'classifier__C': 0.0001, 'selection__k': 7}\",\n",
       " \"{'classifier__C': 0.0001, 'selection__k': 9}\",\n",
       " \"{'classifier__C': 0.0001, 'selection__k': 11}\",\n",
       " \"{'classifier__C': 0.0001, 'selection__k': 13}\",\n",
       " \"{'classifier__C': 0.0001, 'selection__k': 17}\",\n",
       " \"{'classifier__C': 0.0001, 'selection__k': 19}\",\n",
       " \"{'classifier__C': 0.001, 'selection__k': 2}\",\n",
       " \"{'classifier__C': 0.001, 'selection__k': 3}\",\n",
       " \"{'classifier__C': 0.001, 'selection__k': 5}\",\n",
       " \"{'classifier__C': 0.001, 'selection__k': 7}\",\n",
       " \"{'classifier__C': 0.001, 'selection__k': 9}\",\n",
       " \"{'classifier__C': 0.001, 'selection__k': 11}\",\n",
       " \"{'classifier__C': 0.001, 'selection__k': 13}\",\n",
       " \"{'classifier__C': 0.001, 'selection__k': 17}\",\n",
       " \"{'classifier__C': 0.001, 'selection__k': 19}\",\n",
       " \"{'classifier__C': 0.01, 'selection__k': 2}\",\n",
       " \"{'classifier__C': 0.01, 'selection__k': 3}\",\n",
       " \"{'classifier__C': 0.01, 'selection__k': 5}\",\n",
       " \"{'classifier__C': 0.01, 'selection__k': 7}\",\n",
       " \"{'classifier__C': 0.01, 'selection__k': 9}\",\n",
       " \"{'classifier__C': 0.01, 'selection__k': 11}\",\n",
       " \"{'classifier__C': 0.01, 'selection__k': 13}\",\n",
       " \"{'classifier__C': 0.01, 'selection__k': 17}\",\n",
       " \"{'classifier__C': 0.01, 'selection__k': 19}\",\n",
       " \"{'classifier__C': 0.1, 'selection__k': 2}\",\n",
       " \"{'classifier__C': 0.1, 'selection__k': 3}\",\n",
       " \"{'classifier__C': 0.1, 'selection__k': 5}\",\n",
       " \"{'classifier__C': 0.1, 'selection__k': 7}\",\n",
       " \"{'classifier__C': 0.1, 'selection__k': 9}\",\n",
       " \"{'classifier__C': 0.1, 'selection__k': 11}\",\n",
       " \"{'classifier__C': 0.1, 'selection__k': 13}\",\n",
       " \"{'classifier__C': 0.1, 'selection__k': 17}\",\n",
       " \"{'classifier__C': 0.1, 'selection__k': 19}\",\n",
       " \"{'classifier__C': 1, 'selection__k': 2}\",\n",
       " \"{'classifier__C': 1, 'selection__k': 3}\",\n",
       " \"{'classifier__C': 1, 'selection__k': 5}\",\n",
       " \"{'classifier__C': 1, 'selection__k': 7}\",\n",
       " \"{'classifier__C': 1, 'selection__k': 9}\",\n",
       " \"{'classifier__C': 1, 'selection__k': 11}\",\n",
       " \"{'classifier__C': 1, 'selection__k': 13}\",\n",
       " \"{'classifier__C': 1, 'selection__k': 17}\",\n",
       " \"{'classifier__C': 1, 'selection__k': 19}\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.__str__() for param in model.cv_results_[\"params\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
